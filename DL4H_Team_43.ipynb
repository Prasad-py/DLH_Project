{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmAc-DF1nt22"
      },
      "source": [
        "# Reproduction of DeepMicro: Deep Representation Learning for Disease Prediction based on Microbiome Data\n",
        "#### Team Members\n",
        "- Joaquin Ugarte (jugarte2@illinois.edu)\n",
        "- Prasad Gole (gole2@illinois.edu)\n",
        "- Ehit Agarwal (ehitda2@illinois.edu)\n",
        "\n",
        "#### GitHub Repository\n",
        "https://github.com/Prasad-py/DLH_Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "The gut microbiome is a vital component of human health.$^1$ Recent evidence suggests that it can be used to predict various diseases.$^2$ 16S rRNA gene sequencing technology can be used to profile the most common components of the human microbiome in a cost-effective way.$^3$ Similarly, a deeper, strain level, resolution profile of the microbial community can be obtained by shotgun metagenomic sequencing technology.$^4$ As the cost of obtaining microbiome data decreases, there is a novel opportunity for machine learning techniques to be employed for the early prediction of diseases. Detecting diseases early will not only decrease healthcare costs but also improve patient outcomes.\n",
        "\n",
        "Microbiome datasets often contain samples in the range of 10-1000, while the data itself can have hundreds of thousands of dimensions. This poses a challenge to train machine learning models directly on the highly sparse data. The large number of features are computationally expensive and the relatively low number of samples makes the model less generalizabile to other datasets. As of April 2024, state-of-the-art techniques in this field make use of Conditional Generative Adversarial Networks (C-GANs)$^5$ to artificially augment the size of the dataset and Variational Information Bottlenecks (VIBs)$^{6,7}$ to extract only the relevant features for disease prediction while filtering out redundant information.\n",
        "\n",
        "At the time of publication of the DeepMicro study$^8$, there had been little work on deep learning applications for microbiome data with a rigorous evaluation scheme. DeepMicro transforms high-dimensional microbiome data into a robust low-dimensional representation using an autoencoder and then applies machine learning classification on the learned representation. A thorough validation scheme optimizes hyper-parameters using a grid search, where the test set is excluded during cross-validation to ensure fairness. DeepMicro outperforms the current best approaches based on the strain-level marker profile$^9$ in five datasets, including IBD (AUC=0.955), EW-T2D (AUC=0.899), C-T2D (AUC=0.763), Obesity (AUC=0.659) and Cirrhosis (AUC=0.940). For the Colorectal dataset, DeepMicro has slightly lower performance than the best approach (DeepMicro's AUC=0.803 vs. MetAML's AUC=0.811) Additionally, reducing the dimensionality has sped up model training and hyperparameter tuning buy 8-30 times.$^8$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "- Hypothesis 1: Training classifiers using a lower dimensional representation will result in more accurate predictions, as evaluated by area under the ROC.\n",
        "- Hypothesis 2: Training classifiers using a lower dimensional representation will speed up the model training and hyperparameter tuning process.\n",
        "\n",
        "The DeepMicro paper thoroughly describes their procedures. The datasets contain about 1000 samples in total and the datasets are all publically available on their GitHub repository$^{10}$. The models are implemented in a clear and straightforward manner. There is no mention of time required for training or evaluation in the paper, however the computations took about 2 hours on our slightly more powerful machine. For these reasons, the scope of replicating the paper and testing the hypotheses is expected to be high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "In the early stages of our project a DAE autoencoder and an MLP classifier has been used on the IDB cohort to test the hypotheses. The IBD cohort contains 25 IBD patients and 85 healthy controls. The dataset is split into a balanced training set containing 88 samples (80%). A shallow autoencoders with 64 hidden units is trained using strain-level marker profile. The original dataset along with the encoded dataset is used to train 2 MLP models with 2 hidden layers each, containing 32 and 16 units. The models are evaluated on the remaining 22 samples (20%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Packages for unused portions of code\n",
        "'''\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# importing sklearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn import cluster\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "# importing util libraries\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "import sys\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "Publicly available human gut metagenomic samples from six disease cohorts (inflammatory bowel disease, type 2 diabetes, obesity, liver cirrhosis, and colorectal cancer) are obtained from the MetAML study. Data can downloaded from the DeepMicro GitHub repository.$^{10}$ Train/test split is chosen to be 80/20. The abundance and marker datasets are preprocessed accordingly and the features and label are extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# A dict to select a dataset\n",
        "datasets = {\n",
        "    0: 'abundance_Cirrhosis.txt',\n",
        "    1: 'abundance_Colorectal.txt',\n",
        "    2: 'abundance_IBD.txt',\n",
        "    3: 'abundance_Obesity.txt',\n",
        "    4: 'abundance_T2D.txt',\n",
        "    5: 'abundance_WT2D.txt',\n",
        "    6: 'marker_Cirrhosis.txt',\n",
        "    7: 'marker_Colorectal.txt',\n",
        "    8: 'marker_IBD.txt',\n",
        "    9: 'marker_Obesity.txt',\n",
        "    10: 'marker_T2D.txt',\n",
        "    11: 'marker_WT2D.txt'\n",
        "}\n",
        "dataset_file = 'data/' + datasets[8]\n",
        "\n",
        "# Train/test split is 80/20\n",
        "test_size = 0.2\n",
        "\n",
        "# Load the dataset into a dataframe\n",
        "def load_raw_data(dataset_file):\n",
        "    with open(dataset_file, 'r') as fo:\n",
        "        df = pd.read_csv(fo, sep='\\t', header=None, index_col=0, low_memory=False)\n",
        "        return df\n",
        "\n",
        "raw_data = load_raw_data(dataset_file)\n",
        "\n",
        "# Preprocessing for abundance and marker datasets\n",
        "feature_string = ''\n",
        "file_name = dataset_file.split('/')[-1].split('.')[0]\n",
        "if file_name.split('_')[0] == 'abundance':\n",
        "    feature_string = 'k__'\n",
        "elif file_name.split('_')[0] == 'marker':\n",
        "    feature_string = 'gi|'\n",
        "\n",
        "# Compute descriptive statistics\n",
        "def calculate_stats(raw_data):\n",
        "    if file_name.split('_')[1] == 'Obesity':\n",
        "        negative_samples = raw_data.loc['disease'].value_counts()['leaness']\n",
        "    else:\n",
        "        negative_samples = raw_data.loc['disease'].value_counts()['n']\n",
        "    positive_samples = raw_data.shape[1] - negative_samples\n",
        "\n",
        "    print(file_name)\n",
        "    print(f'\\tTotal Samples: {raw_data.shape[1]}')\n",
        "    print(f'\\tPositive Samples: {positive_samples}')\n",
        "    print(f'\\tHealthy Controls: {negative_samples}')\n",
        "    print(f'\\tFraction of Positive Samples: {round(positive_samples/raw_data.shape[1], 3)}')\n",
        "    print(f'\\tNumber of Features: {raw_data.index.str.contains(feature_string, regex=False).size}')\n",
        "\n",
        "calculate_stats(raw_data)\n",
        "\n",
        "# Extract features and labels into tensors\n",
        "def process_data(raw_data):\n",
        "    label_dict = {\n",
        "        # Controls\n",
        "        'n': 0,\n",
        "        # Chirrhosis\n",
        "        'cirrhosis': 1,\n",
        "        # Colorectal Cancer\n",
        "        'cancer': 1, 'small_adenoma': 0,\n",
        "        # IBD\n",
        "        'ibd_ulcerative_colitis': 1, 'ibd_crohn_disease': 1,\n",
        "        # T2D and WT2D\n",
        "        't2d': 1,\n",
        "        # Obesity\n",
        "        'leaness': 0, 'obesity': 1,\n",
        "    }\n",
        "\n",
        "    X = raw_data.loc[raw_data.index.str.contains(feature_string, regex=False)].T\n",
        "    y = raw_data.loc['disease']\n",
        "    y = y.replace(label_dict)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X.values.astype('float64'), y.values.astype('int'), test_size=test_size, stratify=y.values)\n",
        "\n",
        "    return torch.Tensor(X_train), torch.Tensor(X_test), torch.Tensor(y_train), torch.Tensor(y_test)\n",
        "\n",
        "processed_data = process_data(raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "\n",
        "There is an Autoencoder model and a Classifier model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vjx7ERWVkF5"
      },
      "source": [
        "### Autoencoders\n",
        "\n",
        "4 types of autoencoders will be used in the project: Shallow Autoencoder, Deep Autoencoder, Convolutional Autoenconder and Variational Autoenconder. Although all the autoencoders have been prepared, currently only SAE and DAE have been implemented (SAE is a special case of DAE). The encoder contains 3 layers with 256, 128 and 64 units and the decoder is symmetric with 128, 256 and 'input_dim' units. ReLU is the activation function for all layers except the last layer, which uses Sigmoid. MSE is the loss function for the abundance data, while BCE is used for the marker data. Adam is used as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N7ommRMVjWf"
      },
      "outputs": [],
      "source": [
        "# Deep AutoEncoder\n",
        "class DAE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32, degree=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = torch.nn.Sequential()\n",
        "        # First layer\n",
        "        self.encoder.append(torch.nn.Linear(input_dim, hidden_dim*2**degree))\n",
        "        self.encoder.append(torch.nn.ReLU())\n",
        "\n",
        "        # Iteratively add layers to the encoder\n",
        "        for i in range(degree, 0, -1):\n",
        "            self.encoder.append(torch.nn.Linear(hidden_dim*2**i, hidden_dim*2**(i - 1)))\n",
        "            self.encoder.append(torch.nn.ReLU())\n",
        "\n",
        "        # Iteratively add layers to the decoder\n",
        "        self.decoder = torch.nn.Sequential()\n",
        "        for i in range(degree):\n",
        "            self.decoder.append(torch.nn.Linear(hidden_dim*2**i, hidden_dim*2**(i + 1)))\n",
        "            self.decoder.append(torch.nn.ReLU())\n",
        "\n",
        "        # Last layer\n",
        "        self.decoder.append(torch.nn.Linear(hidden_dim*2**degree, input_dim))\n",
        "        self.decoder.append(torch.nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i4BXAeXnt3H"
      },
      "outputs": [],
      "source": [
        "# CAE and VAE have not been implemented yet\n",
        "'''\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, channels, kernel_sizes, strides, paddings, latent_dims):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential()\n",
        "        for i in range(len(channels) - 1):\n",
        "            self.encoder.add_module(f\"conv_{i}\", nn.Conv2d(channels[i], channels[i+1], kernel_sizes[i], strides[i], paddings[i]))\n",
        "            self.encoder.add_module(f\"relu_{i}\", nn.ReLU(True))\n",
        "\n",
        "        self.decoder = nn.Sequential()\n",
        "        rev_channels = channels[::-1]\n",
        "        rev_kernel_sizes = kernel_sizes[::-1]\n",
        "        rev_strides = strides[::-1]\n",
        "        rev_paddings = paddings[::-1]\n",
        "        for i in range(len(rev_channels) - 1):\n",
        "            self.decoder.add_module(f\"deconv_{i}\", nn.ConvTranspose2d(rev_channels[i], rev_channels[i+1], rev_kernel_sizes[i], rev_strides[i], rev_paddings[i]))\n",
        "            self.decoder.add_module(f\"relu_{i}\", nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, latent_dims, device='cpu'):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.device = torch.device(device)\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1 = nn.Linear(input_dims, hidden_dims).to(self.device)\n",
        "        self.fc2_mean = nn.Linear(hidden_dims, latent_dims).to(self.device)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dims, latent_dims).to(self.device)\n",
        "        self.fc3 = nn.Linear(latent_dims, hidden_dims).to(self.device)\n",
        "        self.fc4 = nn.Linear(hidden_dims, input_dims).to(self.device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adjust x if necessary (handled outside the model)\n",
        "        mean, logvar = self.encode(x.to(self.device))\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z), mean, logvar\n",
        "\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Assuming x is your input tensor and it originally has a shape compatible with [batch_size, 200]\n",
        "    # You need to ensure recon_x and x have the same shape for BCE calculation\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # Calculation of KL Divergence remains the same\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCpyJHwnt3H"
      },
      "source": [
        "### Classifier\n",
        "\n",
        "The project uses 3 types of classifiers: Multi-Layer Perceptron, Support Vector Machines and Random Forests. SVM and RFs have been prepared but not implemented yet. The MLP contains two hidden layers with 32 and 16 units, while the output layer has only 1 unit. The activation function for the hidden layers is ReLU, while Sigmoid is used for the output layer. BCE is the loss function and Adam is the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rol6bilynt3H"
      },
      "outputs": [],
      "source": [
        "# Multi-Layer Perceptron\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32, num_layers=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = torch.nn.Sequential()\n",
        "        # Input layer\n",
        "        self.net.append(torch.nn.Linear(input_dim, hidden_dim))\n",
        "        self.net.append(torch.nn.ReLU())\n",
        "        # Iteratively add hidden layers\n",
        "        for i in range(num_layers - 1):\n",
        "            self.net.append(torch.nn.Linear(hidden_dim, int(hidden_dim/2)))\n",
        "            self.net.append(torch.nn.ReLU())\n",
        "            hidden_dim = int(hidden_dim/2)\n",
        "        # Output layer\n",
        "        self.net.append(torch.nn.Linear(hidden_dim, 1))\n",
        "        self.net.append(torch.nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yeLDVz-nt3H"
      },
      "outputs": [],
      "source": [
        "# SVM and RFs have not been implemented yet\n",
        "'''\n",
        "# run exp function\n",
        "def run_exp(seed):\n",
        "    # Initialize the DeepMicrobiome instance\n",
        "    data_file = args.data + '.txt' if args.data else args.custom_data\n",
        "    dm = DeepMicrobiome(data=data_file, seed=seed, data_dir=args.data_dir)\n",
        "            # Load data based on the specified dataset type\n",
        "    if args.data:\n",
        "        feature_string = \"k__\" if \"abundance\" in args.data else \"gi|\" if \"marker\" in args.data else ''\n",
        "        dm.loadData(feature_string=feature_string, label_string='disease', label_dict=label_dict, dtype=dtypeDict[args.dataType])\n",
        "    elif args.custom_data:\n",
        "        if args.custom_data_labels:\n",
        "            dm.loadCustomDataWithLabels(label_data=args.custom_data_labels, dtype=dtypeDict[args.dataType])\n",
        "        else:\n",
        "            dm.loadCustomData(dtype=dtypeDict[args.dataType])\n",
        "    else:\n",
        "        print(\"[Error] No input file specified. Use -h for help.\")\n",
        "        exit()\n",
        "            # Representation learning (Dimensionality reduction)\n",
        "    if args.pca:\n",
        "        dm.pca()\n",
        "    if args.ae:\n",
        "        dm.ae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss,\n",
        "              latent_act=args.ae_lact, output_act=args.ae_oact, patience=args.patience, no_trn=args.no_trn)\n",
        "    if args.vae:\n",
        "        dm.vae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "               patience=25 if args.patience == 20 else args.patience, beta=args.vae_beta, warmup=args.vae_warmup, warmup_rate=args.vae_warmup_rate, no_trn=args.no_trn)\n",
        "    if args.cae:\n",
        "        dm.cae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "               patience=args.patience, rf_rate=args.rf_rate, st_rate=args.st_rate, no_trn=args.no_trn)\n",
        "    if args.rp:\n",
        "        dm.rp()\n",
        "            # Write the learned representation to a file if required\n",
        "    if args.save_rep and (args.pca or args.ae or args.rp or args.vae or args.cae):\n",
        "        rep_file = f\"{dm.data_dir}results/{dm.prefix}{dm.data}_rep.csv\"\n",
        "        X_train_flat = dm.X_train.view(dm.X_train.size(0), -1)  # or you could use numpy: dm.X_train.numpy().reshape(80, -1)\n",
        "        # Convert the flattened tensor to a numpy array and then to a DataFrame\n",
        "        X_train_df = pd.DataFrame(X_train_flat.numpy())\n",
        "        # Save the DataFrame to CSV\n",
        "        X_train_df.to_csv(rep_file, header=None, index=None)\n",
        "        print(f\"The learned representation of the training set has been saved in '{rep_file}'\")\n",
        "    else:\n",
        "        print(\"Warning: No representation learning performed, so nothing was saved.\")\n",
        "\n",
        "def classification(self, hyper_parameters, method='svm', cv=5, scoring='roc_auc', n_jobs=1, cache_size=10000):\n",
        "    clf_start_time = time.time()\n",
        "    # Convert PyTorch tensors to numpy arrays for sklearn models\n",
        "    X_train_np = self.X_train.cpu().detach().numpy()\n",
        "    y_train_np = self.y_train.cpu().detach().numpy()\n",
        "    X_test_np = self.X_test.cpu().detach().numpy()\n",
        "    y_test_np = self.y_test.cpu().detach().numpy()\n",
        "    print(\"# Tuning hyper-parameters\")\n",
        "    print(X_train_np.shape, y_train_np.shape)\n",
        "\n",
        "    if method == 'all' or method == 'svm_rf':\n",
        "        methods = ['svm', 'rf'] if method == 'svm_rf' else ['svm', 'rf', 'mlp']\n",
        "    else:\n",
        "        methods = [method]\n",
        "\n",
        "    for m in methods:\n",
        "        if m == 'svm':\n",
        "            clf = GridSearchCV(SVC(probability=True, cache_size=cache_size), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "        elif m == 'rf':\n",
        "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "        elif m == 'mlp':\n",
        "            model = KerasClassifier(build_fn=mlp_model, input_dim=X_train_np.shape[1], verbose=0)\n",
        "            clf = GridSearchCV(estimator=model, param_grid=hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "\n",
        "        print(f\"Best parameters set found on development set for {m}:\", clf.best_params_)\n",
        "        y_pred = clf.predict(X_test_np)\n",
        "        y_prob = clf.predict_proba(X_test_np)[:, 1] if m != 'mlp' else clf.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "        metrics = [round(roc_auc_score(y_test_np, y_prob), 4),\n",
        "                   round(accuracy_score(y_test_np, y_pred), 4),\n",
        "                   round(recall_score(y_test_np, y_pred), 4),\n",
        "                   round(precision_score(y_test_np, y_pred), 4),\n",
        "                   round(f1_score(y_test_np, y_pred), 4)]\n",
        "\n",
        "        print(f'Metrics for {m} [AUC, ACC, Recall, Precision, F1]:', metrics)\n",
        "\n",
        "        # Save metrics to a file\n",
        "        metrics.append(str(datetime.datetime.now()))\n",
        "        metrics.append(round((time.time() - self.t_start), 2))\n",
        "        metrics.append(round((time.time() - clf_start_time), 2))\n",
        "        metrics.append(str(clf.best_params_))\n",
        "\n",
        "        res = pd.DataFrame([metrics], index=[self.prefix + m])\n",
        "        with open(os.path.join(self.data_dir, \"results\", f\"{self.data}_result.txt\"), 'a') as f:\n",
        "            res.to_csv(f, header=None)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzmUBHNSXp7O"
      },
      "source": [
        "### DeepMicro Framework\n",
        "\n",
        "The DeepMicro class is used for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C1hVrTJXyX1"
      },
      "outputs": [],
      "source": [
        "class DeepMicro():\n",
        "    def __init__(self, processed_data):\n",
        "        # Load preprocessed data into tensors\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = processed_data\n",
        "\n",
        "    def encode(self, hidden_dim=32, degree=0, rho=0.90):\n",
        "        # Create AE object\n",
        "        ae = DAE(input_dim=self.X_train.shape[1], hidden_dim=hidden_dim, degree=degree)\n",
        "        # Select loss function according to dataset\n",
        "        if file_name.split('_')[0] == 'abundance':\n",
        "            loss_func = torch.nn.MSELoss()\n",
        "        elif file_name.split('_')[0] == 'marker':\n",
        "            loss_func = torch.nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(ae.parameters())\n",
        "        # Train the AE\n",
        "        print('Training AutoEncoder')\n",
        "        ae.train()\n",
        "        losses = []\n",
        "        for i in range(31):\n",
        "            optimizer.zero_grad()\n",
        "            X_hat = ae(self.X_train)\n",
        "            loss = loss_func(X_hat, self.X_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.detach().numpy())\n",
        "            # Early Stopping\n",
        "            if i%5 == 0:\n",
        "                print(f'Epoch {i} Loss: {loss}')\n",
        "                X_hat = ae(self.X_test)\n",
        "                val_loss = loss_func(X_hat, self.X_test)\n",
        "                if loss < rho*val_loss and i > 5:\n",
        "                    break\n",
        "\n",
        "        # Update features\n",
        "        self.X_train = ae.encoder(self.X_train).detach()\n",
        "        self.X_test = ae.encoder(self.X_test).detach()\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def classify(self, hidden_dim=32, num_layers=1, rho=0.99):\n",
        "        # Create classifier object\n",
        "        self.clf = MLP(input_dim=self.X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers)\n",
        "        loss_func = torch.nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(self.clf.parameters(), lr=1e-4)\n",
        "        # Train the classifier\n",
        "        print('Training Classifier')\n",
        "        self.clf.train()\n",
        "        losses = []\n",
        "        min_val_loss = 1e10\n",
        "        for i in range(501):\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = self.clf(self.X_train)\n",
        "            loss = loss_func(y_hat.squeeze(dim=-1), self.y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.detach().numpy())\n",
        "            # Early Stopping\n",
        "            if i%5 == 0:\n",
        "                print(f'Epoch {i} Loss: {loss}')\n",
        "                y_hat = self.clf(self.X_test)\n",
        "                val_loss = loss_func(y_hat.squeeze(-1), self.y_test)\n",
        "                if val_loss < rho*min_val_loss:\n",
        "                    min_val_loss = val_loss\n",
        "                elif i >= 50:\n",
        "                    break\n",
        "        return losses\n",
        "\n",
        "    def evaluate(self):\n",
        "        # Evaluate on test set with ROC AUC\n",
        "        self.clf.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat = self.clf(self.X_test).squeeze(-1)\n",
        "            y_pred = (y_hat > 0.5).type(torch.int)\n",
        "            auc = round(roc_auc_score(self.y_test, y_hat), 4)\n",
        "            print(f'\\tROC AUC: {auc}')\n",
        "        return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TURbUoVTnt3H"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "dm = DeepMicro(processed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5C2DixZnt3H"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Classify directly on the features\n",
        "clf_losses = dm.classify(32, 2)\n",
        "clf_auc = dm.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEjSWDJEnt3H"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train the AutoEncoder and update the feature representation\n",
        "en_losses = dm.encode(64, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBAT41BNnt3H"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Classify using the encoded representation\n",
        "en_clf_losses = dm.classify(32, 2)\n",
        "en_clf_auc = dm.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "source": [
        "# Results\n",
        "\n",
        "The results of evaluating the models on the test set are displayed below. Area under the Reciever Operating Characterisitic is used as the evaluation metric. Figure 1. shows the training loss for the autoencoder and Figure 2. shows the training loss for the 2 classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sKJgieZnt3H"
      },
      "outputs": [],
      "source": [
        "# Print AUC metric\n",
        "print(f'AUC for classifier: {clf_auc}')\n",
        "print(f'AUC for encoder-classifier: {en_clf_auc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCrf1ZP-nt3H"
      },
      "outputs": [],
      "source": [
        "# Plot loss for Autoencoder\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.title('Figure 1. AE Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(en_losses, label='AutoEncoder')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj2t8Mcjnt3H"
      },
      "outputs": [],
      "source": [
        "# Plot loss for Classifiers\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.title('Figure 2. CLF vs. En-CLF')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(clf_losses, label='Classifier')\n",
        "plt.plot(en_clf_losses, label='Encoder-Classifier')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "The training loss after encoding is consistently higher than the training loss from running the classifier directly on the dataset. The area under the ROC curve is also consistently higher without using the encoder. Moreover, although the classifier trains faster on the learned representation, the total time taken for training both the encoder and the classifier is much higher than just using the classifier. These results are the opposite of the findings of the original study. The following points discuss possible explanations for these results.\n",
        "\n",
        "- Selection of the best hyperparameters using cross-validation and grid search has not been implemented yet in our project. As a result the models are inferior to those in the DeepMicro study.\n",
        "- CAE, VAE, SVM and RFs have also not been implemented yet. Their implementation could greatly improve performance of the models.\n",
        "- It appears that the encoder trains well but the MLP classifier does not train well with the encoded features (training loss does not decrease by much). The original study suggests that RFs work best with a DAE for the IBD cohort.\n",
        "- The DeepMicro study makes use of machine learning models in Keras, whereas our models are implemented in PyTorch. Hyperparameters related to model training (e.g. learning rate and early stopping) need to be customized for each dataset. These tasks are handled by Keras automatically.\n",
        "\n",
        "After the complete implementation of the project, it is expected to replicate the results of the original study.\n",
        "\n",
        "It can be seen that the models run in under a minute. The computational requirements for this project are low. The entire computation of the DeepMirco study took about 2 hours on our machine. Even without a GPU we expect to finish all computations in a reasonable time period (less than 24 hours).\n",
        "\n",
        "Implementing all the models in PyTorch was a straightforward task. Tuning the hyperparameters manually is a difficult task and needs to be automated. Additionally, we need to deterimine how to best decide on the training specific hyperparameters.\n",
        "\n",
        "Going forward we plan to implement all the missing models and a proper hyperparameter search. Additionally, we plan to add new datasets obtained from the phylaGAN$^5$ and MV-CVIB$^6$ studies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "1. Cho, I. & Blaser, M. J. The human microbiome: at the interface of health and disease. Nature Reviews Genetics 13, 260 (2012).\n",
        "2. Eloe-Fadrosh, E. A. & Rasko, D. A. The human microbiome: from symbiosis to pathogenesis. Annual review of medicine 64, 145-163 (2013).\n",
        "3. Hamady, M. & Knight, R. Microbial community profiling for human microbiome projects: tools, techniques, and challenges. Genome research 19, 1141-1152 (2009).\n",
        "4. Scholz, M. et al. Strain-level microbial epidemiology and population genomics from shotgun metagenomics. Nature methods 13, 435 (2016).\n",
        "5. Divya Sharma, Wendy Lou, Wei Xu, phylaGAN: Data augmentation through conditional GANs and autoencoders for improving disease prediction accuracy using microbiome data, Bioinformatics, 2024;, btae161, https://doi.org/10.1093/bioinformatics/btae161\n",
        "6. Cui Z, Wu Y, Zhang Q-H, Wang S-G, He Y and Huang D-S (2023) MV-CVIB: a microbiome-based multi-view convolutional variational information bottleneck for predicting metastatic colorectal cancer. Front. Microbiol. 14:1238199. doi: 10.3389/fmicb.2023.1238199\n",
        "7. U. Gülfem Elgün Çiftcioğlu, O. Ufuk Nalbanoglu, DeepGum: Deep feature transfer for gut microbiome analysis using bottleneck models, Biomedical Signal Processing and Control, Volume 91, 2024, 105984, ISSN 1746-8094, https://doi.org/10.1016/j.bspc.2024.105984.\n",
        "8. Oh, Min, and Liqing Zhang. \"DeepMicro: deep representation learning for disease prediction based on microbiome data.\" *Scientific Reports* 10.1 (2020): 1-9. https://doi.org/10.1038/s41598-020-63159-5.\n",
        "9. Pasolli, E., Truong, D. T., Malik, F., Waldron, L. & Segata, N. Machine learning meta-analysis of large metagenomic datasets: tools and biological insights. PLoS computational biology 12, e1004977 (2016).\n",
        "10. https://github.com/minoh0201/DeepMicro"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}