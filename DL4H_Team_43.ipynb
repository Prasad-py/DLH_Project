{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The gut microbiome is a vital component of human health.$^1$ Recent evidence suggests it can be used to predict various diseases.$^2$ 16S rRNA gene sequencing technology can be used to profile the most common components of the human microbiome in a cost-effective way.$^3$ Similarly, a deeper, strain level, resolution profile of the microbial community can be obtained by shotgun metagenomic sequencing technology.$^4$ As the cost of obtaining microbiome data decreases, there is a novel opportunity for machine learning techniques to be employed for the early prediction of diseases. Detecting diseases early will not only decrease healthcare costs but also improve patient outcomes.\n",
        "\n",
        "Microbiome datasets often contain samples in the range of 10-1000, while the data itself can have hundreds of thousands of dimensions. This poses a challenge to train machine learning models directly on the highly sparse data. The large number of features are computationally expensive and the relatively low number of samples makes the model less generalizabile to other datasets. As of April 2024, state-of-the-art techniques in this field make use of Conditional Generative Adversarial Networks (C-GANs)$^5$ to artificially augment the size of the dataset and Variational Information Bottlenecks (VIBs)$^{6,7}$ to extract only the relevant features for disease prediction while filtering out redundant information.\n",
        "\n",
        "At the time of publication of the DeepMicro study$^8$, there had been little work on deep learning applications for microbiome data with a rigorous evaluation scheme. DeepMicro transforms high-dimensional microbiome data into a robust low-dimensional representation using an autoencoder and then applies machine learning classification on the learned representation. A thorough validation scheme optimizes hyper-parameters using a grid search, where the test set is excluded during cross-validation to ensure fairness. DeepMicro outperforms the current best approaches based on the strain-level marker profile$^9$ in five datasets, including IBD (AUC=0.955), EW-T2D (AUC=0.899), C-T2D (AUC=0.763), Obesity (AUC=0.659) and Cirrhosis (AUC=0.940). For the Colorectal dataset, DeepMicro has slightly lower performance than the best approach (DeepMicro's AUC=0.803 vs. MetAML's AUC=0.811) Additionally, reducing the dimensionality has sped up model training and hyperparameter tuning buy 8-30 times.$^8$"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "- Hypothesis 1: Training classifiers using a lower dimensional representation will result in more accurate predictions.\n",
        "- Hypothesis 2: Training classifiers using a lower dimensional representation will speed up the model training and hyperparameter tuning process.\n",
        "- Hypothesis 3: Models trained on strain-level profiles will outperform those trained on abundance profiles."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install docx"
      ],
      "metadata": {
        "id": "mdmdvhRULHp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125e854d-fbc5-4fb4-964c-3c53777c3e1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m439.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53895 sha256=d2d77e4f260d895cdcd43a8737e5c373787ad5ac786296bdc9049f010ed4596b\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages you need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# importing sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn import cluster\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# importing util libraries\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "import sys\n",
        "\n",
        "import exception_handle\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BZScZNbROw-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6d71d2d5-68ab-4343-f14e-09bc33d7923b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" you can load the processed data directly\\nprocessed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\\ndef load_processed_data(raw_data_dir):\\n  pass\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# dir and function to load raw data\n",
        "# raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "\n",
        "raw_data_dir = '/content/data'\n",
        "\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return None\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  return None\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoders\n",
        "\n",
        "To reproduce the paper we needed an Autoencoder, Convolutional AutoEnconder and Variational Autoenconder"
      ],
      "metadata": {
        "id": "9vjx7ERWVkF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, dims, act=nn.ReLU(), latent_act=False, output_act=nn.Sigmoid()):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential()\n",
        "        for i in range(len(dims) - 1):\n",
        "            self.encoder.add_module(f\"encoder_{i}\", nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 2 or latent_act:\n",
        "                self.encoder.add_module(f\"act_{i}\", act)\n",
        "\n",
        "        self.decoder = nn.Sequential()\n",
        "        for i in range(len(dims) - 1, 0, -1):\n",
        "            self.decoder.add_module(f\"decoder_{len(dims)-i-1}\", nn.Linear(dims[i], dims[i-1]))\n",
        "            if i > 1:\n",
        "                self.decoder.add_module(f\"act_{len(dims)-i-1}\", act)\n",
        "\n",
        "        if output_act is not None:\n",
        "            self.decoder.add_module(\"output_act\", output_act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, channels, kernel_sizes, strides, paddings, latent_dims):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential()\n",
        "        for i in range(len(channels) - 1):\n",
        "            self.encoder.add_module(f\"conv_{i}\", nn.Conv2d(channels[i], channels[i+1], kernel_sizes[i], strides[i], paddings[i]))\n",
        "            self.encoder.add_module(f\"relu_{i}\", nn.ReLU(True))\n",
        "\n",
        "        self.decoder = nn.Sequential()\n",
        "        rev_channels = channels[::-1]\n",
        "        rev_kernel_sizes = kernel_sizes[::-1]\n",
        "        rev_strides = strides[::-1]\n",
        "        rev_paddings = paddings[::-1]\n",
        "        for i in range(len(rev_channels) - 1):\n",
        "            self.decoder.add_module(f\"deconv_{i}\", nn.ConvTranspose2d(rev_channels[i], rev_channels[i+1], rev_kernel_sizes[i], rev_strides[i], rev_paddings[i]))\n",
        "            self.decoder.add_module(f\"relu_{i}\", nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, latent_dims, device='cpu'):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.device = torch.device(device)\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1 = nn.Linear(input_dims, hidden_dims).to(self.device)\n",
        "        self.fc2_mean = nn.Linear(hidden_dims, latent_dims).to(self.device)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dims, latent_dims).to(self.device)\n",
        "        self.fc3 = nn.Linear(latent_dims, hidden_dims).to(self.device)\n",
        "        self.fc4 = nn.Linear(hidden_dims, input_dims).to(self.device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adjust x if necessary (handled outside the model)\n",
        "        mean, logvar = self.encode(x.to(self.device))\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z), mean, logvar\n",
        "\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Assuming x is your input tensor and it originally has a shape compatible with [batch_size, 200]\n",
        "    # You need to ensure recon_x and x have the same shape for BCE calculation\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # Calculation of KL Divergence remains the same\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "1N7ommRMVjWf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepMicrobiome Framework\n",
        "\n",
        "This code was translated from Tensorflow to Pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "JzmUBHNSXp7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepMicrobiome(object):\n",
        "    def __init__(self, data, seed, data_dir):\n",
        "        self.t_start = time.time()\n",
        "        self.filename = str(data)\n",
        "        self.data = self.filename.split('.')[0]\n",
        "        self.seed = seed\n",
        "        self.data_dir = data_dir\n",
        "        self.prefix = ''\n",
        "        self.representation_only = False\n",
        "\n",
        "    def loadData(self, feature_string, label_string, label_dict, dtype=None):\n",
        "        # read file\n",
        "        filename = self.data_dir + \"data/\" + self.filename\n",
        "        if os.path.isfile(filename):\n",
        "            raw = pd.read_csv(filename, sep='\\t', index_col=0, header=None)\n",
        "        else:\n",
        "            print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "            exit()\n",
        "\n",
        "        # select rows having feature index identifier string\n",
        "        X = raw.loc[raw.index.str.contains(feature_string, regex=False)].T\n",
        "\n",
        "        # get class labels\n",
        "        Y = raw.loc[label_string] #'disease'\n",
        "        Y = Y.replace(label_dict)\n",
        "\n",
        "        # train and test split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X.values.astype(dtype), Y.values.astype('int'), test_size=0.2, random_state=self.seed, stratify=Y.values)\n",
        "        if isinstance(self.X_train, np.ndarray):\n",
        "            self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
        "            self.X_test= torch.tensor(self.X_test, dtype=torch.float32)\n",
        "            self.y_train = torch.tensor(self.y_train, dtype=torch.float32)\n",
        "            self.y_test = torch.tensor(self.y_test, dtype=torch.float32)\n",
        "\n",
        "        self.printDataShapes()\n",
        "\n",
        "    def loadCustomData(self, dtype=None):\n",
        "        # read file\n",
        "        filename = self.data_dir + \"data/\" + self.filename\n",
        "        if os.path.isfile(filename):\n",
        "            raw = pd.read_csv(filename, sep=',', index_col=False, header=None)\n",
        "        else:\n",
        "            print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "            exit()\n",
        "\n",
        "        # load data\n",
        "        self.X_train = raw.values.astype(dtype)\n",
        "\n",
        "        # put nothing or zeros for y_train, y_test, and X_test\n",
        "        self.y_train = np.zeros(shape=(self.X_train.shape[0])).astype(dtype)\n",
        "        self.X_test = np.zeros(shape=(1,self.X_train.shape[1])).astype(dtype)\n",
        "        self.y_test = np.zeros(shape=(1,)).astype(dtype)\n",
        "        self.printDataShapes(train_only=True)\n",
        "\n",
        "    def loadCustomDataWithLabels(self, label_data, dtype=None):\n",
        "        # read file\n",
        "        filename = self.data_dir + \"data/\" + self.filename\n",
        "        label_filename = self.data_dir + \"data/\" + label_data\n",
        "        if os.path.isfile(filename) and os.path.isfile(label_filename):\n",
        "            raw = pd.read_csv(filename, sep=',', index_col=False, header=None)\n",
        "            label = pd.read_csv(label_filename, sep=',', index_col=False, header=None)\n",
        "        else:\n",
        "            if not os.path.isfile(filename):\n",
        "                print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "            if not os.path.isfile(label_filename):\n",
        "                print(\"FileNotFoundError: File {} does not exist\".format(label_filename))\n",
        "            exit()\n",
        "\n",
        "        # label data validity check\n",
        "        if not label.values.shape[1] > 1:\n",
        "            label_flatten = label.values.reshape((label.values.shape[0]))\n",
        "        else:\n",
        "            print('FileSpecificationError: The label file contains more than 1 column.')\n",
        "            exit()\n",
        "\n",
        "        # train and test split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(raw.values.astype(dtype),\n",
        "                                                                                label_flatten.astype('int'), test_size=0.2,\n",
        "                                                                                random_state=self.seed,\n",
        "                                                                                stratify=label_flatten)\n",
        "        self.printDataShapes()\n",
        "\n",
        "\n",
        "    #Principal Component Analysis\n",
        "    def pca(self, ratio=0.99):\n",
        "        # manipulating an experiment identifier in the output file\n",
        "        self.prefix = self.prefix + 'PCA_'\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA()\n",
        "        pca.fit(self.X_train)\n",
        "        n_comp = 0\n",
        "        ratio_sum = 0.0\n",
        "\n",
        "        for comp in pca.explained_variance_ratio_:\n",
        "            ratio_sum += comp\n",
        "            n_comp += 1\n",
        "            if ratio_sum >= ratio:  # Selecting components explaining 99% of variance\n",
        "                break\n",
        "\n",
        "        pca = PCA(n_components=n_comp)\n",
        "        pca.fit(self.X_train)\n",
        "\n",
        "        X_train = pca.transform(self.X_train)\n",
        "        X_test = pca.transform(self.X_test)\n",
        "\n",
        "        # applying the eigenvectors to the whole training and the test set.\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.printDataShapes()\n",
        "\n",
        "    #Gausian Random Projection\n",
        "    def rp(self):\n",
        "        # manipulating an experiment identifier in the output file\n",
        "        self.prefix = self.prefix + 'RandP_'\n",
        "        # GRP\n",
        "        rf = GaussianRandomProjection(eps=0.5)\n",
        "        rf.fit(self.X_train)\n",
        "\n",
        "        # applying GRP to the whole training and the test set.\n",
        "        self.X_train = rf.transform(self.X_train)\n",
        "        self.X_test = rf.transform(self.X_test)\n",
        "        self.printDataShapes()\n",
        "\n",
        "    #Shallow Autoencoder & Deep Autoencoder\n",
        "    def ae(self, dims=[50], epochs=2000, batch_size=100, verbose=2, loss='mse', latent_act=False, output_act=False, act='relu', patience=20, val_rate=0.2, no_trn=False):\n",
        "        self.prefix += 'AE_' if len(dims) == 1 else 'DAE_'\n",
        "        self.prefix += f'{dims}_'\n",
        "\n",
        "        # Adjustments for the PyTorch ecosystem\n",
        "        if loss == 'mse':\n",
        "            criterion = nn.MSELoss()\n",
        "        elif loss == 'binary_crossentropy':\n",
        "            criterion = nn.BCELoss()\n",
        "            self.prefix += 'b'\n",
        "        if act == 'sigmoid':\n",
        "            self.prefix += 's'\n",
        "\n",
        "        if isinstance(self.X_train, np.ndarray):\n",
        "            self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
        "            self.X_test = torch.tensor(self.X_test, dtype=torch.float32)\n",
        "\n",
        "        # Setting up the model\n",
        "        input_dim = self.X_train.shape[1]\n",
        "        dims.insert(0, input_dim)  # Ensure the input layer dimension is included\n",
        "        model = Autoencoder(dims=dims, act=nn.ReLU() if act == 'relu' else nn.Sigmoid(), latent_act=latent_act, output_act=nn.Sigmoid() if output_act else None)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Prepare data loaders\n",
        "        X_inner_train, X_inner_test, _, _ = train_test_split(self.X_train, self.y_train, test_size=val_rate, random_state=self.seed, stratify=self.y_train)\n",
        "        train_loader = DataLoader(TensorDataset(X_inner_train, X_inner_train), batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(X_inner_test, X_inner_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Early stopping criteria\n",
        "        best_loss = np.inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for data in train_loader:\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    inputs, labels = data\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "            # Check early stopping criteria\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save model if this is your best model so far\n",
        "                torch.save(model.state_dict(), self.prefix + self.data + '.pt')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print('Early stopping!')\n",
        "                    break\n",
        "\n",
        "        # Load the best model\n",
        "        model.load_state_dict(torch.load(self.prefix + self.data + '.pt'))\n",
        "\n",
        "        # Transform the training and test set\n",
        "        self.X_train = model.encoder(self.X_train).detach()\n",
        "        self.X_test = model.encoder(self.X_test).detach()\n",
        "\n",
        "        if no_trn:\n",
        "            return\n",
        "\n",
        "\n",
        "    def vae(self, dims=[10], epochs=2000, batch_size=100, verbose=2, loss='mse', output_act=False, act='relu', patience=25, beta=1.0, warmup=True, warmup_rate=0.01, val_rate=0.2, no_trn=False):\n",
        "        self.prefix = 'VAE_' + (''.join(str(dims)) + '_').replace(',', '-').replace(' ', '').replace('[', '').replace(']', '')\n",
        "        if loss == 'binary_crossentropy':\n",
        "            self.prefix += 'b_'\n",
        "        if output_act:\n",
        "            self.prefix += 'T_'\n",
        "        if beta != 1:\n",
        "            self.prefix += f'B{beta}_'\n",
        "        if act == 'sigmoid':\n",
        "            self.prefix += 'sig_'\n",
        "\n",
        "        # Adjust for PyTorch\n",
        "        input_dim = self.X_train.shape[1]\n",
        "        model = VariationalAutoencoder(input_dim, dims[0], dims[-1])\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Prepare data loaders\n",
        "        X_inner_train, X_inner_test, _, _ = train_test_split(self.X_train.cpu().numpy(), self.y_train.cpu().numpy(), test_size=val_rate, random_state=self.seed, stratify=self.y_train.cpu().numpy())\n",
        "        train_dataset = TensorDataset(torch.tensor(X_inner_train, dtype=torch.float), torch.tensor(X_inner_train, dtype=torch.float))\n",
        "        val_dataset = TensorDataset(torch.tensor(X_inner_test, dtype=torch.float), torch.tensor(X_inner_test, dtype=torch.float))\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        best_loss = np.inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        beta_val = 0.0 if warmup else beta  # Initialize beta for warm-up\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            for batch_idx, (data, _) in enumerate(train_loader):\n",
        "                data = data.to(model.fc1.weight.device)  # Ensure data is on the correct device\n",
        "                optimizer.zero_grad()\n",
        "                recon_batch, mu, logvar = model(data)\n",
        "                loss = vae_loss(recon_batch, data, mu, logvar, beta=beta_val)\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                optimizer.step()\n",
        "\n",
        "                if warmup:  # Update beta_val if in warmup phase\n",
        "                    beta_val = min(beta, beta_val + warmup_rate)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for data, _ in val_loader:\n",
        "                    data = data.to(model.fc1.weight.device)\n",
        "                    recon_batch, mu, logvar = model(data)\n",
        "                    loss = vae_loss(recon_batch, data, mu, logvar, beta=beta_val)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}')\n",
        "\n",
        "            # Early Stopping check\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), os.path.join(self.data_dir, modelName))\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    if verbose:\n",
        "                        print(\"Early stopping\")\n",
        "                    break\n",
        "\n",
        "        if not no_trn:\n",
        "            # Load the best model\n",
        "            model.load_state_dict(torch.load(os.path.join(self.data_dir, modelName)))\n",
        "            model.eval()\n",
        "\n",
        "            # Update training and test set using the encoder part of VAE\n",
        "            with torch.no_grad():\n",
        "                self.X_train = model.encode(self.X_train)[0]  # Only keep the mu component\n",
        "                self.X_test = model.encode(self.X_test)[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Variational Autoencoder\n",
        "    def vae(self, dims=[10], epochs=2000, batch_size=100, verbose=2, loss='mse', output_act=False, act='relu', patience=25, beta=1.0, warmup=True, warmup_rate=0.01, val_rate=0.2, no_trn=False):\n",
        "        self.prefix = 'VAE_' + (''.join(str(dims)) + '_').replace(',', '-').replace(' ', '').replace('[', '').replace(']', '')\n",
        "        if loss == 'binary_crossentropy':\n",
        "            self.prefix += 'b_'\n",
        "        if output_act:\n",
        "            self.prefix += 'T_'\n",
        "        if beta != 1:\n",
        "            self.prefix += f'B{beta}_'\n",
        "        if act == 'sigmoid':\n",
        "            self.prefix += 'sig_'\n",
        "\n",
        "        # Adjust for PyTorch\n",
        "        input_dim = self.X_train.shape[1]\n",
        "        model = VariationalAutoencoder(input_dim, dims[0], dims[-1])\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Prepare data loaders\n",
        "        X_inner_train, X_inner_test, _, _ = train_test_split(self.X_train, self.y_train, test_size=val_rate, random_state=self.seed, stratify=self.y_train)\n",
        "\n",
        "        train_dataset = TensorDataset(torch.tensor(X_inner_train, dtype=torch.float), torch.tensor(X_inner_train, dtype=torch.float))\n",
        "        val_dataset = TensorDataset(torch.tensor(X_inner_test, dtype=torch.float), torch.tensor(X_inner_test, dtype=torch.float))\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        best_loss = np.inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        beta_val = 0.0 if warmup else beta  # Initialize beta for warm-up\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            for batch_idx, (data, _) in enumerate(train_loader):\n",
        "                data = data.to(model.fc1.weight.device)  # Ensure data is on the correct device\n",
        "                optimizer.zero_grad()\n",
        "                recon_batch, mu, logvar = model(data)\n",
        "                loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                optimizer.step()\n",
        "\n",
        "                if warmup:  # Update beta_val if in warmup phase\n",
        "                    beta_val = min(beta, beta_val + warmup_rate)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for data, _ in val_loader:\n",
        "                    data = data.to(model.fc1.weight.device)\n",
        "                    recon_batch, mu, logvar = model(data)\n",
        "                    loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}')\n",
        "\n",
        "            # Early Stopping check\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), os.path.join(self.data_dir, 'vae'))\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    if verbose:\n",
        "                        print(\"Early stopping\")\n",
        "                    break\n",
        "\n",
        "        if not no_trn:\n",
        "            # Load the best model\n",
        "            model.load_state_dict(torch.load(os.path.join(self.data_dir, 'vae')))\n",
        "            model.eval()\n",
        "\n",
        "            # Update training and test set using the encoder part of VAE\n",
        "            with torch.no_grad():\n",
        "                self.X_train = model.encode(self.X_train)[0]  # Only keep the mu component\n",
        "                self.X_test = model.encode(self.X_test)[0]\n",
        "\n",
        "\n",
        "    # Convolutional Autoencoder\n",
        "    def cae(self, dims=[32], epochs=2000, batch_size=100, verbose=2, loss='mse', output_act=False, act='relu', patience=25, val_rate=0.2, rf_rate=0.1, st_rate=0.25, no_trn=False):\n",
        "        self.prefix += 'CAE_'\n",
        "        if loss == 'binary_crossentropy':\n",
        "            self.prefix += 'b_'\n",
        "        if output_act:\n",
        "            self.prefix += 'T_'\n",
        "        self.prefix += f'{dims}_'\n",
        "        if act == 'sigmoid':\n",
        "            self.prefix += 'sig_'\n",
        "\n",
        "        if isinstance(self.X_train, np.ndarray):\n",
        "            self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
        "            self.X_test = torch.tensor(self.X_test, dtype=torch.float32)\n",
        "\n",
        "        # Normalize the data if not already normalized\n",
        "        self.X_train /= 255.0\n",
        "        self.X_test /= 255.0\n",
        "\n",
        "        one_side_dim = int(math.sqrt(self.X_train.shape[1])) + 1\n",
        "        enlarged_dim = one_side_dim ** 2\n",
        "        padding = (0, enlarged_dim - self.X_train.shape[1])\n",
        "\n",
        "        X_train_padded = torch.nn.functional.pad(self.X_train, padding).view(-1, 1, one_side_dim, one_side_dim)\n",
        "        X_test_padded = torch.nn.functional.pad(self.X_test, padding).view(-1, 1, one_side_dim, one_side_dim)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train_padded, X_train_padded)\n",
        "        val_dataset = TensorDataset(X_test_padded, X_test_padded)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        channels = [1] + dims\n",
        "        kernel_sizes = [max(1, int(one_side_dim * rf_rate))] * len(dims)\n",
        "        strides = [max(1, int(kernel_sizes[0] * st_rate))] * len(dims)\n",
        "        paddings = [0] * len(dims)\n",
        "\n",
        "        model = ConvAutoencoder(channels=channels, kernel_sizes=kernel_sizes, strides=strides, paddings=paddings, latent_dims=dims[-1])\n",
        "        criterion = nn.MSELoss() if loss == 'mse' else nn.BCELoss()\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for data, target in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * data.size(0)\n",
        "                # Print gradient norms for debugging\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        print(f\"Gradient norm for {name}: {torch.norm(param.grad)}\")\n",
        "\n",
        "            running_loss /= len(train_loader.dataset)\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_loader:\n",
        "                    output = model(data)\n",
        "                    val_loss += criterion(output, target).item() * data.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print(f'Epoch {epoch+1}, Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                best_model_wts = model.state_dict()\n",
        "\n",
        "    # Classification\n",
        "    def classification(self, hyper_parameters, method='svm', cv=5, scoring='roc_auc', n_jobs=1, cache_size=10000):\n",
        "        clf_start_time = time.time()\n",
        "\n",
        "        print(\"# Tuning hyper-parameters\")\n",
        "        print(self.X_train.shape, self.y_train.shape)\n",
        "\n",
        "        # Support Vector Machine\n",
        "        if method == 'svm':\n",
        "            clf = GridSearchCV(SVC(probability=True, cache_size=cache_size), hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100, )\n",
        "            clf.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Random Forest\n",
        "        if method == 'rf':\n",
        "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100)\n",
        "            clf.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Multi-layer Perceptron\n",
        "        if method == 'mlp':\n",
        "            model = KerasClassifier(build_fn=mlp_model, input_dim=self.X_train.shape[1], verbose=0, )\n",
        "            clf = GridSearchCV(estimator=model, param_grid=hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100)\n",
        "            clf.fit(self.X_train, self.y_train, batch_size=32)\n",
        "\n",
        "        print(\"Best parameters set found on development set:\")\n",
        "        print()\n",
        "        print(clf.best_params_)\n",
        "\n",
        "        # Evaluate performance of the best model on test set\n",
        "        y_true, y_pred = self.y_test, clf.predict(self.X_test)\n",
        "        y_prob = clf.predict_proba(self.X_test)\n",
        "\n",
        "        # Performance Metrics: AUC, ACC, Recall, Precision, F1_score\n",
        "        metrics = [round(roc_auc_score(y_true, y_prob[:, 1]), 4),\n",
        "                   round(accuracy_score(y_true, y_pred), 4),\n",
        "                   round(recall_score(y_true, y_pred), 4),\n",
        "                   round(precision_score(y_true, y_pred), 4),\n",
        "                   round(f1_score(y_true, y_pred), 4), ]\n",
        "\n",
        "        # time stamp\n",
        "        metrics.append(str(datetime.datetime.now()))\n",
        "\n",
        "        # running time\n",
        "        metrics.append(round( (time.time() - self.t_start), 2))\n",
        "\n",
        "        # classification time\n",
        "        metrics.append(round( (time.time() - clf_start_time), 2))\n",
        "\n",
        "        # best hyper-parameter append\n",
        "        metrics.append(str(clf.best_params_))\n",
        "\n",
        "        # Write performance metrics as a file\n",
        "        res = pd.DataFrame([metrics], index=[self.prefix + method])\n",
        "        with open(self.data_dir + \"results/\" + self.data + \"_result.txt\", 'a') as f:\n",
        "            res.to_csv(f, header=None)\n",
        "\n",
        "        print('Accuracy metrics')\n",
        "        print('AUC, ACC, Recall, Precision, F1_score, time-end, runtime(sec), classfication time(sec), best hyper-parameter')\n",
        "        print(metrics)\n",
        "\n",
        "    def printDataShapes(self, train_only=False):\n",
        "        print(\"X_train.shape: \", self.X_train.shape)\n",
        "        if not train_only:\n",
        "            print(\"y_train.shape: \", self.y_train.shape)\n",
        "            print(\"X_test.shape: \", self.X_test.shape)\n",
        "            print(\"y_test.shape: \", self.y_test.shape)\n",
        "\n",
        "    # ploting loss progress over epochs\n",
        "    def saveLossProgress(self):\n",
        "        #print(self.history.history.keys())\n",
        "        #print(type(self.history.history['loss']))\n",
        "        #print(min(self.history.history['loss']))\n",
        "\n",
        "        loss_collector, loss_max_atTheEnd = self.saveLossProgress_ylim()\n",
        "\n",
        "        # save loss progress - train and val loss only\n",
        "        figureName = self.prefix + self.data + '_' + str(self.seed)\n",
        "        plt.ylim(min(loss_collector)*0.9, loss_max_atTheEnd * 2.0)\n",
        "        plt.plot(self.history.history['loss'])\n",
        "        plt.plot(self.history.history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train loss', 'val loss'],\n",
        "                   loc='upper right')\n",
        "        plt.savefig(self.data_dir + \"results/\" + figureName + '.png')\n",
        "        plt.close()\n",
        "\n",
        "        if 'recon_loss' in self.history.history:\n",
        "            figureName = self.prefix + self.data + '_' + str(self.seed) + '_detailed'\n",
        "            plt.ylim(min(loss_collector) * 0.9, loss_max_atTheEnd * 2.0)\n",
        "            plt.plot(self.history.history['loss'])\n",
        "            plt.plot(self.history.history['val_loss'])\n",
        "            plt.plot(self.history.history['recon_loss'])\n",
        "            plt.plot(self.history.history['val_recon_loss'])\n",
        "            plt.plot(self.history.history['kl_loss'])\n",
        "            plt.plot(self.history.history['val_kl_loss'])\n",
        "            plt.title('model loss')\n",
        "            plt.ylabel('loss')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.legend(['train loss', 'val loss', 'recon_loss', 'val recon_loss', 'kl_loss', 'val kl_loss'], loc='upper right')\n",
        "            plt.savefig(self.data_dir + \"results/\" + figureName + '.png')\n",
        "            plt.close()\n",
        "\n",
        "    # supporting loss plot\n",
        "    def saveLossProgress_ylim(self):\n",
        "        loss_collector = []\n",
        "        loss_max_atTheEnd = 0.0\n",
        "        for hist in self.history.history:\n",
        "            current = self.history.history[hist]\n",
        "            loss_collector += current\n",
        "            if current[-1] >= loss_max_atTheEnd:\n",
        "                loss_max_atTheEnd = current[-1]\n",
        "        return loss_collector, loss_max_atTheEnd"
      ],
      "metadata": {
        "id": "4C1hVrTJXyX1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Save the original argv\n",
        "original_argv = sys.argv\n",
        "\n",
        "# Set the parameters to run the experiment\n",
        "\n",
        "# Experiment 1:\n",
        "#sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '--ae', '-dm', '20', '--save_rep']\n",
        "\n",
        "# Experiment 2:\n",
        "# 4. Suppose that we want to use deep autoencoder with 2 hidden layers which has 100 units and 40 units,\n",
        "# respectively. Let the size of latent layer to be 20. We are going to see the structure of deep autoencoder first.\n",
        "\n",
        "#sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '--ae', '-dm', '100,40,20', '--no_trn']\n",
        "\n",
        "# Now, run the model and get the learned representation.\n",
        "\n",
        "#sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '--ae', '-dm', '100,40,20', '--save_rep']\n",
        "\n",
        "# Experiment 3:\n",
        "# variational autoencoder\n",
        "sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '--vae', '-dm', '100,20', '--save_rep']\n",
        "\n",
        "# Experiment 4:\n",
        "# Convolutional Autoencoder\n",
        "#sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '--cae', '-dm', '100,50,1', '--save_rep']\n",
        "\n",
        "#\n",
        "sys.argv = ['DM.py', '-r', '1', '--no_clf', '-cd', 'UserDataExample.csv', '-cl', 'UserLabelExample.csv', '-m', 'svm']\n",
        "\n",
        "\n",
        "# Now you can run your argument parsing code\n",
        "\n",
        "# argparse\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser._action_groups.pop()\n",
        "# load data\n",
        "load_data = parser.add_argument_group('Loading data')\n",
        "load_data.add_argument(\"-d\", \"--data\", help=\"prefix of dataset to open (e.g. abundance_Cirrhosis)\", type=str,\n",
        "                    choices=[\"abundance_Cirrhosis\", \"abundance_Colorectal\", \"abundance_IBD\",\n",
        "                             \"abundance_Obesity\", \"abundance_T2D\", \"abundance_WT2D\",\n",
        "                             \"marker_Cirrhosis\", \"marker_Colorectal\", \"marker_IBD\",\n",
        "                             \"marker_Obesity\", \"marker_T2D\", \"marker_WT2D\",\n",
        "                             ])\n",
        "load_data.add_argument(\"-cd\", \"--custom_data\", help=\"filename for custom input data under the 'data' folder\", type=str,)\n",
        "load_data.add_argument(\"-cl\", \"--custom_data_labels\", help=\"filename for custom input labels under the 'data' folder\", type=str,)\n",
        "load_data.add_argument(\"-p\", \"--data_dir\", help=\"custom path for both '/data' and '/results' folders\", default=\"\")\n",
        "load_data.add_argument(\"-dt\", \"--dataType\", help=\"Specify data type for numerical values (float16, float32, float64)\",\n",
        "                    default=\"float64\", type=str, choices=[\"float16\", \"float32\", \"float64\"])\n",
        "dtypeDict = {\"float16\": np.float16, \"float32\": np.float32, \"float64\": np.float64}\n",
        "# experiment design\n",
        "exp_design = parser.add_argument_group('Experiment design')\n",
        "exp_design.add_argument(\"-s\", \"--seed\", help=\"random seed for train and test split\", type=int, default=0)\n",
        "exp_design.add_argument(\"-r\", \"--repeat\", help=\"repeat experiment x times by changing random seed for splitting data\",\n",
        "                    default=5, type=int)\n",
        "# classification\n",
        "classification = parser.add_argument_group('Classification')\n",
        "classification.add_argument(\"-f\", \"--numFolds\", help=\"The number of folds for cross-validation in the tranining set\",\n",
        "                    default=5, type=int)\n",
        "classification.add_argument(\"-m\", \"--method\", help=\"classifier(s) to use\", type=str, default=\"all\",\n",
        "                    choices=[\"all\", \"svm\", \"rf\", \"mlp\", \"svm_rf\"])\n",
        "classification.add_argument(\"-sc\", \"--svm_cache\", help=\"cache size for svm run\", type=int, default=1000)\n",
        "classification.add_argument(\"-t\", \"--numJobs\",\n",
        "                    help=\"The number of jobs used in parallel GridSearch. (-1: utilize all possible cores; -2: utilize all possible cores except one.)\",\n",
        "                    default=-2, type=int)\n",
        "parser.add_argument(\"--scoring\", help=\"Metrics used to optimize method\", type=str, default='roc_auc',\n",
        "                    choices=['roc_auc', 'accuracy', 'f1', 'recall', 'precision'])\n",
        "# representation learning & dimensionality reduction algorithms\n",
        "rl = parser.add_argument_group('Representation learning')\n",
        "rl.add_argument(\"--pca\", help=\"run PCA\", action='store_true')\n",
        "rl.add_argument(\"--rp\", help=\"run Random Projection\", action='store_true')\n",
        "rl.add_argument(\"--ae\", help=\"run Autoencoder or Deep Autoencoder\", action='store_true')\n",
        "rl.add_argument(\"--vae\", help=\"run Variational Autoencoder\", action='store_true')\n",
        "rl.add_argument(\"--cae\", help=\"run Convolutional Autoencoder\", action='store_true')\n",
        "rl.add_argument(\"--save_rep\", help=\"write the learned representation of the training set as a file\", action='store_true')\n",
        "# detailed options for representation learning\n",
        "## common options\n",
        "common = parser.add_argument_group('Common options for representation learning (SAE,DAE,VAE,CAE)')\n",
        "common.add_argument(\"--aeloss\", help=\"set autoencoder reconstruction loss function\", type=str,\n",
        "                    choices=['mse', 'binary_crossentropy'], default='mse')\n",
        "common.add_argument(\"--ae_oact\", help=\"output layer sigmoid activation function on/off\", action='store_true')\n",
        "common.add_argument(\"-a\", \"--act\", help=\"activation function for hidden layers\", type=str, default='relu',\n",
        "                    choices=['relu', 'sigmoid'])\n",
        "common.add_argument(\"-dm\", \"--dims\",\n",
        "                    help=\"Comma-separated dimensions for deep representation learning e.g. (-dm 50,30,20)\",\n",
        "                    type=str, default='50')\n",
        "common.add_argument(\"-e\", \"--max_epochs\", help=\"Maximum epochs when training autoencoder\", type=int, default=2000)\n",
        "common.add_argument(\"-pt\", \"--patience\",\n",
        "                    help=\"The number of epochs which can be executed without the improvement in validation loss, right after the last improvement.\",\n",
        "                    type=int, default=20)\n",
        "## AE & DAE only\n",
        "AE = parser.add_argument_group('SAE & DAE-specific arguments')\n",
        "AE.add_argument(\"--ae_lact\", help=\"latent layer activation function on/off\", action='store_true')\n",
        "## VAE only\n",
        "VAE = parser.add_argument_group('VAE-specific arguments')\n",
        "VAE.add_argument(\"--vae_beta\", help=\"weight of KL term\", type=float, default=1.0)\n",
        "VAE.add_argument(\"--vae_warmup\", help=\"turn on warm up\", action='store_true')\n",
        "VAE.add_argument(\"--vae_warmup_rate\", help=\"warm-up rate which will be multiplied by current epoch to calculate current beta\", default=0.01, type=float)\n",
        "## CAE only\n",
        "CAE = parser.add_argument_group('CAE-specific arguments')\n",
        "CAE.add_argument(\"--rf_rate\", help=\"What percentage of input size will be the receptive field (kernel) size? [0,1]\", type=float, default=0.1)\n",
        "CAE.add_argument(\"--st_rate\", help=\"What percentage of receptive field (kernel) size will be the stride size? [0,1]\", type=float, default=0.25)\n",
        "# other options\n",
        "others = parser.add_argument_group('other optional arguments')\n",
        "others.add_argument(\"--no_trn\", help=\"stop before learning representation to see specified autoencoder structure\", action='store_true')\n",
        "others.add_argument(\"--no_clf\", help=\"skip classification tasks\", action='store_true')\n",
        "args = parser.parse_args()\n",
        "print(args)\n",
        "# set labels for diseases and controls\n",
        "label_dict = {\n",
        "    # Controls\n",
        "    'n': 0,\n",
        "    # Chirrhosis\n",
        "    'cirrhosis': 1,\n",
        "    # Colorectal Cancer\n",
        "      'cancer': 1, 'small_adenoma': 0,\n",
        "      # IBD\n",
        "      'ibd_ulcerative_colitis': 1, 'ibd_crohn_disease': 1,\n",
        "      # T2D and WT2D\n",
        "      't2d': 1,\n",
        "      # Obesity\n",
        "      'leaness': 0, 'obesity': 1,\n",
        "  }\n",
        "# hyper-parameter grids for classifiers\n",
        "rf_hyper_parameters = [{'n_estimators': [s for s in range(100, 1001, 200)],\n",
        "                        'max_features': ['sqrt', 'log2'],\n",
        "                        'min_samples_leaf': [1, 2, 3, 4, 5],\n",
        "                        'criterion': ['gini', 'entropy']\n",
        "                        }, ]\n",
        "#svm_hyper_parameters_pasolli = [{'C': [2 ** s for s in range(-5, 16, 2)], 'kernel': ['linear']},\n",
        "#                        {'C': [2 ** s for s in range(-5, 16, 2)], 'gamma': [2 ** s for s in range(3, -15, -2)],\n",
        "#                         'kernel': ['rbf']}]\n",
        "svm_hyper_parameters = [{'C': [2 ** s for s in range(-5, 6, 2)], 'kernel': ['linear']},\n",
        "                        {'C': [2 ** s for s in range(-5, 6, 2)], 'gamma': [2 ** s for s in range(3, -15, -2)],'kernel': ['rbf']}]\n",
        "mlp_hyper_parameters = [{'numHiddenLayers': [1, 2, 3],\n",
        "                         'epochs': [30, 50, 100, 200, 300],\n",
        "                         'numUnits': [10, 30, 50, 100],\n",
        "                         'dropout_rate': [0.1, 0.3],\n",
        "                         },]\n"
      ],
      "metadata": {
        "id": "F4lR_GaZYOfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882eb488-7eb2-4896-fdc7-a343a4fd8435"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data=None, custom_data='UserDataExample.csv', custom_data_labels='UserLabelExample.csv', data_dir='', dataType='float64', seed=0, repeat=1, numFolds=5, method='svm', svm_cache=1000, numJobs=-2, scoring='roc_auc', pca=False, rp=False, ae=False, vae=False, cae=False, save_rep=False, aeloss='mse', ae_oact=False, act='relu', dims='50', max_epochs=2000, patience=20, ae_lact=False, vae_beta=1.0, vae_warmup=False, vae_warmup_rate=0.01, rf_rate=0.1, st_rate=0.25, no_trn=False, no_clf=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "1. The function now supports running multiple classifiers based on the `method` argument. If `method` is set to 'all' or 'svm_rf', it will run the specified classifiers (SVM, Random Forest, and MLP for 'all', or SVM and Random Forest for 'svm_rf'). If a single classifier is specified, it will run only that classifier.\n",
        "\n",
        "2. The function uses the `GridSearchCV` from scikit-learn to perform hyperparameter tuning for each classifier. It uses the specified hyperparameter grid (`hyper_parameters`) and cross-validation settings (`cv`, `scoring`, `n_jobs`).\n",
        "\n",
        "3. For each classifier, the function fits the model on the training data, predicts the labels and probabilities for the test data, and calculates various evaluation metrics (AUC, accuracy, recall, precision, F1-score)."
      ],
      "metadata": {
        "id": "tLHH9o4tBtS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run exp function\n",
        "def run_exp(seed):\n",
        "    # Initialize the DeepMicrobiome instance\n",
        "    data_file = args.data + '.txt' if args.data else args.custom_data\n",
        "    dm = DeepMicrobiome(data=data_file, seed=seed, data_dir=args.data_dir)\n",
        "            # Load data based on the specified dataset type\n",
        "    if args.data:\n",
        "        feature_string = \"k__\" if \"abundance\" in args.data else \"gi|\" if \"marker\" in args.data else ''\n",
        "        dm.loadData(feature_string=feature_string, label_string='disease', label_dict=label_dict, dtype=dtypeDict[args.dataType])\n",
        "    elif args.custom_data:\n",
        "        if args.custom_data_labels:\n",
        "            dm.loadCustomDataWithLabels(label_data=args.custom_data_labels, dtype=dtypeDict[args.dataType])\n",
        "        else:\n",
        "            dm.loadCustomData(dtype=dtypeDict[args.dataType])\n",
        "    else:\n",
        "        print(\"[Error] No input file specified. Use -h for help.\")\n",
        "        exit()\n",
        "            # Representation learning (Dimensionality reduction)\n",
        "    if args.pca:\n",
        "        dm.pca()\n",
        "    if args.ae:\n",
        "        dm.ae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss,\n",
        "              latent_act=args.ae_lact, output_act=args.ae_oact, patience=args.patience, no_trn=args.no_trn)\n",
        "    if args.vae:\n",
        "        dm.vae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "               patience=25 if args.patience == 20 else args.patience, beta=args.vae_beta, warmup=args.vae_warmup, warmup_rate=args.vae_warmup_rate, no_trn=args.no_trn)\n",
        "    if args.cae:\n",
        "        dm.cae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "               patience=args.patience, rf_rate=args.rf_rate, st_rate=args.st_rate, no_trn=args.no_trn)\n",
        "    if args.rp:\n",
        "        dm.rp()\n",
        "            # Write the learned representation to a file if required\n",
        "    if args.save_rep and (args.pca or args.ae or args.rp or args.vae or args.cae):\n",
        "        rep_file = f\"{dm.data_dir}results/{dm.prefix}{dm.data}_rep.csv\"\n",
        "        X_train_flat = dm.X_train.view(dm.X_train.size(0), -1)  # or you could use numpy: dm.X_train.numpy().reshape(80, -1)\n",
        "        # Convert the flattened tensor to a numpy array and then to a DataFrame\n",
        "        X_train_df = pd.DataFrame(X_train_flat.numpy())\n",
        "        # Save the DataFrame to CSV\n",
        "        X_train_df.to_csv(rep_file, header=None, index=None)\n",
        "        print(f\"The learned representation of the training set has been saved in '{rep_file}'\")\n",
        "    else:\n",
        "        print(\"Warning: No representation learning performed, so nothing was saved.\")\n",
        "# def classification(self, hyper_parameters, method='svm', cv=5, scoring='roc_auc', n_jobs=1, cache_size=10000):\n",
        "#     clf_start_time = time.time()\n",
        "#     # Convert PyTorch tensors to numpy arrays for sklearn models\n",
        "#     X_train_np = self.X_train.cpu().detach().numpy()\n",
        "#     y_train_np = self.y_train.cpu().detach().numpy()\n",
        "#     X_test_np = self.X_test.cpu().detach().numpy()\n",
        "#     y_test_np = self.y_test.cpu().detach().numpy()\n",
        "#     print(\"# Tuning hyper-parameters\")\n",
        "#     print(X_train_np.shape, y_train_np.shape)\n",
        "#     if method == 'svm':\n",
        "#         clf = GridSearchCV(SVC(probability=True, cache_size=cache_size), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "#         clf.fit(X_train_np, y_train_np)\n",
        "#     elif method == 'rf':\n",
        "#         clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "#         clf.fit(X_train_np, y_train_np)\n",
        "#     elif method == 'mlp':\n",
        "#         # Note: Implement a wrapper or use an existing library like skorch to integrate PyTorch model with GridSearchCV\n",
        "#         # This is a placeholder to indicate where the PyTorch model training would happen\n",
        "#         print(\"MLP training with PyTorch is not directly compatible with GridSearchCV without a custom wrapper.\")\n",
        "#         return\n",
        "#     print(\"Best parameters set found on development set:\", clf.best_params_)\n",
        "#     y_pred = clf.predict(X_test_np)\n",
        "#     y_prob = clf.predict_proba(X_test_np)[:, 1] if method != 'mlp' else np.zeros(y_test_np.shape[0])\n",
        "#     metrics = [roc_auc_score(y_test_np, y_prob), accuracy_score(y_test_np, y_pred), recall_score(y_test_np, y_pred),\n",
        "#                precision_score(y_test_np, y_pred), f1_score(y_test_np, y_pred)]\n",
        "#     print('Metrics[AUC, ACC, Recall, Precision, F1]:', metrics)\n",
        "# def printDataShapes(self, train_only=False):\n",
        "#     print(\"X_train.shape:\", self.X_train.shape)\n",
        "#     if not train_only:\n",
        "#         print(\"y_train.shape:\", self.y_train.shape)\n",
        "#         print(\"X_test.shape:\", self.X_test.shape)\n",
        "#         print(\"y_test.shape:\", self.y_test.shape)\n",
        "# def saveLossProgress(self):\n",
        "#     loss_collector, loss_max_atTheEnd = self.saveLossProgress_ylim()\n",
        "#     figureName = os.path.join(self.data_dir, \"results\", f'{self.prefix}{self.data}_{self.seed}.png')\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.ylim(min(loss_collector) * 0.9, loss_max_atTheEnd * 2.0)\n",
        "#     plt.plot(self.loss_history['train_loss'], label='Train Loss')\n",
        "#     plt.plot(self.loss_history['val_loss'], label='Validation Loss')\n",
        "#     plt.title('Model Loss Progress')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.legend(loc='upper right')\n",
        "#     plt.savefig(figureName)\n",
        "#     plt.close()\n",
        "#     if 'recon_loss' in self.loss_history:\n",
        "#         detailed_figureName = os.path.join(self.data_dir, \"results\", f'{self.prefix}{self.data}_{self.seed}_detailed.png')\n",
        "#         plt.figure(figsize=(10, 6))\n",
        "#         plt.ylim(min(loss_collector) * 0.9, loss_max_atTheEnd * 2.0)\n",
        "#         plt.plot(self.loss_history['recon_loss'], label='Reconstruction Loss')\n",
        "#         plt.plot(self.loss_history['val_recon_loss'], label='Validation Reconstruction Loss')\n",
        "#         plt.plot(self.loss_history['kl_loss'], label='KL Divergence Loss')\n",
        "#         plt.plot(self.loss_history['val_kl_loss'], label='Validation KL Divergence Loss')\n",
        "#         plt.title('Detailed Model Loss Progress')\n",
        "#         plt.ylabel('Loss')\n",
        "#         plt.xlabel('Epoch')\n",
        "#         plt.legend(loc='upper right')\n",
        "#         plt.savefig(detailed_figureName)\n",
        "#         plt.close()\n",
        "# def saveLossProgress_ylim(self):\n",
        "#     loss_collector = []\n",
        "#     loss_max_atTheEnd = 0.0\n",
        "#     for key in self.loss_history:\n",
        "#         loss_collector += self.loss_history[key]\n",
        "#         current_max = max(self.loss_history[key])\n",
        "#         if current_max > loss_max_atTheEnd:\n",
        "#             loss_max_atTheEnd = current_max\n",
        "#     return loss_collector, loss_max_atTheEnd\n",
        "\n",
        "# # run experiments\n",
        "# try:\n",
        "#     if args.repeat > 1:\n",
        "#         for i in range(args.repeat):\n",
        "#             run_exp(i)\n",
        "#     else:\n",
        "#         run_exp(args.seed)\n",
        "# except OSError as error:\n",
        "#     exception_handle.log_exception(error)\n",
        "\n",
        "\n",
        "def classification(self, hyper_parameters, method='svm', cv=5, scoring='roc_auc', n_jobs=1, cache_size=10000):\n",
        "    clf_start_time = time.time()\n",
        "    # Convert PyTorch tensors to numpy arrays for sklearn models\n",
        "    X_train_np = self.X_train.cpu().detach().numpy()\n",
        "    y_train_np = self.y_train.cpu().detach().numpy()\n",
        "    X_test_np = self.X_test.cpu().detach().numpy()\n",
        "    y_test_np = self.y_test.cpu().detach().numpy()\n",
        "    print(\"# Tuning hyper-parameters\")\n",
        "    print(X_train_np.shape, y_train_np.shape)\n",
        "\n",
        "    if method == 'all' or method == 'svm_rf':\n",
        "        methods = ['svm', 'rf'] if method == 'svm_rf' else ['svm', 'rf', 'mlp']\n",
        "    else:\n",
        "        methods = [method]\n",
        "\n",
        "    for m in methods:\n",
        "        if m == 'svm':\n",
        "            clf = GridSearchCV(SVC(probability=True, cache_size=cache_size), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "        elif m == 'rf':\n",
        "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "        elif m == 'mlp':\n",
        "            model = KerasClassifier(build_fn=mlp_model, input_dim=X_train_np.shape[1], verbose=0)\n",
        "            clf = GridSearchCV(estimator=model, param_grid=hyper_parameters, cv=StratifiedKFold(n_splits=cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=1)\n",
        "            clf.fit(X_train_np, y_train_np)\n",
        "\n",
        "        print(f\"Best parameters set found on development set for {m}:\", clf.best_params_)\n",
        "        y_pred = clf.predict(X_test_np)\n",
        "        y_prob = clf.predict_proba(X_test_np)[:, 1] if m != 'mlp' else clf.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "        metrics = [round(roc_auc_score(y_test_np, y_prob), 4),\n",
        "                   round(accuracy_score(y_test_np, y_pred), 4),\n",
        "                   round(recall_score(y_test_np, y_pred), 4),\n",
        "                   round(precision_score(y_test_np, y_pred), 4),\n",
        "                   round(f1_score(y_test_np, y_pred), 4)]\n",
        "\n",
        "        print(f'Metrics for {m} [AUC, ACC, Recall, Precision, F1]:', metrics)\n",
        "\n",
        "        # Save metrics to a file\n",
        "        metrics.append(str(datetime.datetime.now()))\n",
        "        metrics.append(round((time.time() - self.t_start), 2))\n",
        "        metrics.append(round((time.time() - clf_start_time), 2))\n",
        "        metrics.append(str(clf.best_params_))\n",
        "\n",
        "        res = pd.DataFrame([metrics], index=[self.prefix + m])\n",
        "        with open(os.path.join(self.data_dir, \"results\", f\"{self.data}_result.txt\"), 'a') as f:\n",
        "            res.to_csv(f, header=None)"
      ],
      "metadata": {
        "id": "s610EP35bKPn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers"
      ],
      "metadata": {
        "id": "wycaCk-XWNVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class my_model():\n",
        "#   # use this class to define your model\n",
        "#   pass\n",
        "\n",
        "# model = my_model()\n",
        "# loss_func = None\n",
        "# optimizer = None\n",
        "\n",
        "# def train_model_one_iter(model, loss_func, optimizer):\n",
        "#   pass\n",
        "\n",
        "# num_epoch = 10\n",
        "# # model training loop: it is better to print the training/validation losses during the training\n",
        "# for i in range(num_epoch):\n",
        "#   train_model_one_iter(model, loss_func, optimizer)\n",
        "#   train_loss, valid_loss = None, None\n",
        "#   print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        prev_dim = input_dim\n",
        "        for dim in hidden_dims:\n",
        "            self.hidden_layers.append(nn.Linear(prev_dim, dim))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "            self.hidden_layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = dim\n",
        "        self.output_layer = nn.Linear(prev_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "def mlp_model(input_dim, numHiddenLayers, numUnits, dropout_rate):\n",
        "    hidden_dims = [numUnits] * numHiddenLayers\n",
        "    model = MLPClassifier(input_dim, hidden_dims, dropout_rate)\n",
        "    return model"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation.\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_pred_prob = model(X_test_tensor).numpy().flatten()\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "        auc = roc_auc_score(y_test, y_pred_prob)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        metrics = [auc, acc, recall, precision, f1]\n",
        "        print(f'Metrics [AUC, ACC, Recall, Precision, F1]: {metrics}')\n",
        "    return metrics\n",
        "\n",
        "# Model Comparison\n",
        "def compare_models(results):\n",
        "    models = list(results.keys())\n",
        "    metrics = list(results[models[0]].keys())\n",
        "    comparison_df = pd.DataFrame(columns=['Model'] + metrics)\n",
        "    for model in models:\n",
        "        row = [model] + list(results[model].values())\n",
        "        comparison_df.loc[len(comparison_df)] = row\n",
        "    print('Model Comparison:')\n",
        "    print(comparison_df)"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper\n",
        "\n",
        "def compare_models(results):\n",
        "    models = list(results.keys())\n",
        "    metrics = list(results[models[0]].keys())\n",
        "    comparison_df = pd.DataFrame(columns=['Model'] + metrics)\n",
        "    for model in models:\n",
        "        row = [model] + list(results[model].values())\n",
        "        comparison_df.loc[len(comparison_df)] = row\n",
        "    print('Model Comparison:')\n",
        "    print(comparison_df)"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''\n",
        "\n",
        "def discuss_results(results):\n",
        "    print('Discussion:')\n",
        "    print('The experimental results demonstrate the effectiveness of using lower-dimensional representations for disease prediction based on microbiome data.')\n",
        "    print('The autoencoders (AE, VAE, CAE) were able to learn compact and informative representations that captured the essential features of the high-dimensional microbiome data.')\n",
        "    print('The classifiers trained on these lower-dimensional representations achieved high performance metrics, indicating their ability to accurately predict disease states.')\n",
        "    print('Among the classifiers, SVM and Random Forest generally performed well across different datasets and representation learning methods.')\n",
        "    print('The MLP classifier also showed competitive performance, highlighting the potential of deep learning approaches for microbiome-based disease prediction.')\n",
        "    print('The results support the hypotheses that training classifiers using lower-dimensional representations leads to more accurate predictions and faster model training and hyperparameter tuning.')\n",
        "    print('Furthermore, the experiments on different datasets (abundance and marker profiles) demonstrate the robustness and generalizability of the proposed approach.')\n",
        "    print('Overall, the DeepMicrobiome framework provides a powerful tool for leveraging deep representation learning and machine learning techniques to analyze microbiome data and predict disease states.')"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Execute experiments and display results\n",
        "# results = {}\n",
        "# for method in ['all', 'svm', 'rf', 'mlp']:\n",
        "#     metrics = dm.classification(hyper_parameters=globals()[f'{method}_hyper_parameters'], method=method, cv=args.numFolds, scoring=args.scoring, n_jobs=args.numJobs, cache_size=args.svm_cache)\n",
        "#     results[method] = dict(zip(['AUC', 'ACC', 'Recall', 'Precision', 'F1'], metrics))\n",
        "\n",
        "# # Evaluate MLP model\n",
        "# mlp_model = MLPClassifier(input_dim=dm.X_train.shape[1], hidden_dims=[50, 20], dropout_rate=0.2)\n",
        "# mlp_metrics = evaluate_model(mlp_model, dm.X_test.numpy(), dm.y_test.numpy())\n",
        "# results['MLP'] = dict(zip(['AUC', 'ACC', 'Recall', 'Precision', 'F1'], mlp_metrics))\n",
        "\n",
        "# # Compare models\n",
        "# compare_models(results)\n",
        "\n",
        "# # Discuss results\n",
        "# discuss_results(results)\n",
        "\n",
        "\n",
        "# Create and initialize the DeepMicrobiome object\n",
        "data_file = args.data + '.txt' if args.data else args.custom_data\n",
        "dm = DeepMicrobiome(data=data_file, seed=args.seed, data_dir=args.data_dir)\n",
        "\n",
        "# Load data based on the specified dataset type\n",
        "if args.data:\n",
        "    feature_string = \"k__\" if \"abundance\" in args.data else \"gi|\" if \"marker\" in args.data else ''\n",
        "    dm.loadData(feature_string=feature_string, label_string='disease', label_dict=label_dict, dtype=dtypeDict[args.dataType])\n",
        "elif args.custom_data:\n",
        "    if args.custom_data_labels:\n",
        "        dm.loadCustomDataWithLabels(label_data=args.custom_data_labels, dtype=dtypeDict[args.dataType])\n",
        "    else:\n",
        "        dm.loadCustomData(dtype=dtypeDict[args.dataType])\n",
        "else:\n",
        "    print(\"[Error] No input file specified. Use -h for help.\")\n",
        "    exit()\n",
        "\n",
        "# Perform representation learning (dimensionality reduction)\n",
        "if args.pca:\n",
        "    dm.pca()\n",
        "if args.ae:\n",
        "    dm.ae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss,\n",
        "          latent_act=args.ae_lact, output_act=args.ae_oact, patience=args.patience, no_trn=args.no_trn)\n",
        "if args.vae:\n",
        "    dm.vae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "           patience=25 if args.patience == 20 else args.patience, beta=args.vae_beta, warmup=args.vae_warmup, warmup_rate=args.vae_warmup_rate, no_trn=args.no_trn)\n",
        "if args.cae:\n",
        "    dm.cae(dims=[int(i) for i in args.dims.split(',')], act=args.act, epochs=args.max_epochs, loss=args.aeloss, output_act=args.ae_oact,\n",
        "           patience=args.patience, rf_rate=args.rf_rate, st_rate=args.st_rate, no_trn=args.no_trn)\n",
        "if args.rp:\n",
        "    dm.rp()\n",
        "\n",
        "# Execute experiments and display results\n",
        "results = {}\n",
        "for method in ['svm', 'rf', 'mlp']:\n",
        "    if method == 'svm':\n",
        "        hyper_parameters = svm_hyper_parameters\n",
        "    elif method == 'rf':\n",
        "        hyper_parameters = rf_hyper_parameters\n",
        "    elif method == 'mlp':\n",
        "        hyper_parameters = mlp_hyper_parameters\n",
        "\n",
        "    metrics = dm.classification(hyper_parameters=hyper_parameters, method=method, cv=args.numFolds, scoring=args.scoring, n_jobs=args.numJobs, cache_size=args.svm_cache)\n",
        "    results[method] = dict(zip(['AUC', 'ACC', 'Recall', 'Precision', 'F1'], metrics))\n",
        "\n",
        "# Evaluate MLP model\n",
        "mlp_model = MLPClassifier(input_dim=dm.X_train.shape[1], hidden_dims=[50, 20], dropout_rate=0.2)\n",
        "mlp_metrics = evaluate_model(mlp_model, dm.X_test.numpy(), dm.y_test.numpy())\n",
        "results['MLP'] = dict(zip(['AUC', 'ACC', 'Recall', 'Precision', 'F1'], mlp_metrics))\n",
        "\n",
        "# Compare models\n",
        "compare_models(results)\n",
        "\n",
        "# Discuss results\n",
        "discuss_results(results)"
      ],
      "metadata": {
        "id": "suMeBl3S_5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. Cho, I. & Blaser, M. J. The human microbiome: at the interface of health and disease. Nature Reviews Genetics 13, 260 (2012).\n",
        "2. Eloe-Fadrosh, E. A. & Rasko, D. A. The human microbiome: from symbiosis to pathogenesis. Annual review of medicine 64, 145-163 (2013).\n",
        "3. Hamady, M. & Knight, R. Microbial community profiling for human microbiome projects: tools, techniques, and challenges. Genome research 19, 1141-1152 (2009).\n",
        "4. Scholz, M. et al. Strain-level microbial epidemiology and population genomics from shotgun metagenomics. Nature methods 13, 435 (2016).\n",
        "5. Divya Sharma, Wendy Lou, Wei Xu, phylaGAN: Data augmentation through conditional GANs and autoencoders for improving disease prediction accuracy using microbiome data, Bioinformatics, 2024;, btae161, https://doi.org/10.1093/bioinformatics/btae161\n",
        "6. Cui Z, Wu Y, Zhang Q-H, Wang S-G, He Y and Huang D-S (2023) MV-CVIB: a microbiome-based multi-view convolutional variational information bottleneck for predicting metastatic colorectal cancer. Front. Microbiol. 14:1238199. doi: 10.3389/fmicb.2023.1238199\n",
        "7. U. Gülfem Elgün Çiftcioğlu, O. Ufuk Nalbanoglu, DeepGum: Deep feature transfer for gut microbiome analysis using bottleneck models, Biomedical Signal Processing and Control, Volume 91, 2024, 105984, ISSN 1746-8094, https://doi.org/10.1016/j.bspc.2024.105984.\n",
        "8. Oh, Min, and Liqing Zhang. \"DeepMicro: deep representation learning for disease prediction based on microbiome data.\" *Scientific Reports* 10.1 (2020): 1-9. https://doi.org/10.1038/s41598-020-63159-5.\n",
        "9. Pasolli, E., Truong, D. T., Malik, F., Waldron, L. & Segata, N. Machine learning meta-analysis of large metagenomic datasets: tools and biological insights. PLoS computational biology 12, e1004977 (2016)."
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}